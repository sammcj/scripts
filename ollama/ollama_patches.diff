diff --git a/llm/ext_server/server.cpp b/llm/ext_server/server.cpp
index 3448bcc..2a2186c 100644
--- a/llm/ext_server/server.cpp
+++ b/llm/ext_server/server.cpp
@@ -2124,6 +2124,7 @@ static void server_print_usage(const char *argv0, const gpt_params &params,
     printf("                            types: int, float, bool. example: --override-kv tokenizer.ggml.add_bos_token=bool:false\n");
     printf("  -gan N, --grp-attn-n N    set the group attention factor to extend context size through self-extend(default: 1=disabled), used together with group attention width `--grp-attn-w`\n");
     printf("  -gaw N, --grp-attn-w N    set the group attention width to extend context size through self-extend(default: 512), used together with group attention factor `--grp-attn-n`\n");
+    printf("  --flash_attn              enable flash attention (default: disabled)\n");
     printf("  --chat-template JINJA_TEMPLATE\n");
     printf("                            set custom jinja chat template (default: template taken from model's metadata)\n");
     printf("                            Note: only commonly used templates are accepted, since we don't have jinja parser\n");
@@ -2333,6 +2334,10 @@ static void server_params_parse(int argc, char **argv, server_params &sparams,

             params.grp_attn_n = std::stoi(argv[i]);
         }
+        else if (arg == "--flash-attn" || arg == "-fa")
+        {
+            params.flash_attn = true;
+        }
         else if (arg == "--grp-attn-w" || arg == "-gaw")
         {
             if (++i >= argc)
